base_config: &BASE_CONFIG
    random_seed: 42
    # log_test_interval: 1      # log the training loss and learning rate every n epochs

transformer: &TRANSFORMER
    <<: *BASE_CONFIG
    # modular sum operation of K numbers
    length: 2
    prime: 97
    training_fraction: 0.5
    optimizer: 'adamw'

    # transformer parameters
    num_layers: 2
    dim_model: 128
    num_heads: 4
    batch_size: 512
    learning_rate: 0.001
    weight_decay: 1
    dropout: 0.0
    epochs: 10000
    momentum: 0.9

transformer_k: &TRANSFORMER_K
    <<: *BASE_CONFIG
    # modular sum operation of K numbers
    length: 2
    prime: 11
    training_fraction: 0.5
    optimizer: 'adamw'

    # transformer parameters
    num_layers: 2
    dim_model: 64
    num_heads: 4
    batch_size: 512
    learning_rate: 0.001
    weight_decay: 1
    dropout: 0.0
    epochs: 10000
    momentum: 0.9

mlp: &MLP
    <<: *BASE_CONFIG
    # modular sum operation of K numbers
    length: 2
    prime: 97
    training_fraction: 0.5
    num_layers: 2
    dim_model: 128
    num_heads: 4
    batch_size: 512
    dropout: 0.0
    epochs: 50000
    learning_rate: 0.01
    weight_decay: 0.01
    momentum: 0.9

lstm: &LSTM
    <<: *BASE_CONFIG
    # modular sum operation of K numbers
    length: 2
    prime: 97
    training_fraction: 0.5

    # LSTM parameters
    num_layers: 2
    dim_model: 128
    hidden_dim: 128
    batch_size: 512
    learning_rate: 0.001
    weight_decay: 1
    dropout: 0.0
    epochs: 10000
    momentum: 0.9
